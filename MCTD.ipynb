{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MCTD**\n",
    "This notebook contains the source code for running a black-box optimization algorithm using the MCTD algorithm.  \n",
    "The notebook was tested on the Google colab platform with GPU accelration [https://research.google.com/colaboratory/faq.html]  \n",
    "The Python version at Google Colab is 3.7.13 at the time of test.  \n",
    "\n",
    "Additionally, it is possible to run the notebook locally by installing the required packages in 'requirement.txt'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukDI_apn78uz"
   },
   "source": [
    "## This section contains the package installation for Google Colab\n",
    "If the notebook is running in a local environment with the necessary packages, this section may be *skipped*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0rd5Yks7zjv"
   },
   "source": [
    "### Install Pytorch, GPytorch, and scipy on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s12y_PK63Py0",
    "outputId": "35c173a9-3dea-4e79-b9cc-abb1ab7842fe"
   },
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip install gpytorch\n",
    "# !pip install --upgrade scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ag0ZltCb8_9-",
    "outputId": "e59ff1ea-2436-476e-b4a8-9c4cbf5dd8a5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numbers\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "import random\n",
    "import math\n",
    "from abc import abstractmethod\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is cuda available: True \n"
     ]
    }
   ],
   "source": [
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore', category=Warning)\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import qmc \n",
    "\n",
    "import torch\n",
    "print('Is cuda available: %s ' % torch.cuda.is_available())\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.constraints.constraints import Interval\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.models import ExactGP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized TuRBO model\n",
    "\n",
    "The original *TuRBO* model source code is from https://github.com/uber-research/TuRBO  \n",
    "Here, we made modifications to the source code to fit our algorithm by May.25.2022\n",
    "\n",
    "The license of original *TuRBO* is as follows [https://github.com/uber-research/TuRBO/blob/master/LICENSE.md]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by the text below.\n",
    " \n",
    "\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.\n",
    " \n",
    "\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\n",
    " \n",
    "\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\n",
    " \n",
    "\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\n",
    " \n",
    "\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under this License.\n",
    " \n",
    "This License governs use of the accompanying Work, and your use of the Work constitutes acceptance of this License.\n",
    " \n",
    "You may use this Work for any non-commercial purpose, subject to the restrictions in this License. Some purposes which can be non-commercial are teaching, academic research, and personal experimentation. You may also distribute this Work with books or other teaching materials, or publish the Work on websites, that are intended to teach the use of the Work.\n",
    " \n",
    "You may not use or distribute this Work, or any derivative works, outputs, or results from the Work, in any form for commercial purposes. Non-exhaustive examples of commercial purposes would be running business operations, licensing, leasing, or selling the Work, or distributing the Work for use with commercial products.\n",
    " \n",
    "You may modify this Work and distribute the modified Work for non-commercial purposes, however, you may not grant rights to the Work or derivative works that are broader than or in conflict with those provided by this License. For example, you may not distribute modifications of the Work under terms that would permit commercial use, or under terms that purport to require the Work or derivative works to be sublicensed to others.\n",
    "\n",
    "In return, we require that you agree:\n",
    "\n",
    "1. Not to remove any copyright or other notices from the Work.\n",
    " \n",
    "2. That if you distribute the Work in Source or Object form, you will include a verbatim copy of this License.\n",
    " \n",
    "3. That if you distribute derivative works of the Work in Source form, you do so only under a license that includes all of the provisions of this License and is not in conflict with this License, and if you distribute derivative works of the Work solely in Object form you do so only under a license that complies with this License.\n",
    " \n",
    "4. That if you have modified the Work or created derivative works from the Work, and distribute such modifications or derivative works, you will cause the modified files to carry prominent notices so that recipients know that they are not receiving the original Work. Such notices must state: (i) that you have changed the Work; and (ii) the date of any changes.\n",
    " \n",
    "5. If you publicly use the Work or any output or result of the Work, you will provide a notice with such use that provides any person who uses, views, accesses, interacts with, or is otherwise exposed to the Work (i) with information of the nature of the Work, (ii) with a link to the Work, and (iii) a notice that the Work is available under this License.\n",
    " \n",
    "6. THAT THE WORK COMES \"AS IS\", WITH NO WARRANTIES. THIS MEANS NO EXPRESS, IMPLIED OR STATUTORY WARRANTY, INCLUDING WITHOUT LIMITATION, WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE OR ANY WARRANTY OF TITLE OR NON-INFRINGEMENT. ALSO, YOU MUST PASS THIS DISCLAIMER ON WHENEVER YOU DISTRIBUTE THE WORK OR DERIVATIVE WORKS.\n",
    " \n",
    "7. THAT NEITHER UBER TECHNOLOGIES, INC. NOR ANY OF ITS AFFILIATES, SUPPLIERS, SUCCESSORS, NOR ASSIGNS WILL BE LIABLE FOR ANY DAMAGES RELATED TO THE WORK OR THIS LICENSE, INCLUDING DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL OR INCIDENTAL DAMAGES, TO THE MAXIMUM EXTENT THE LAW PERMITS, NO MATTER WHAT LEGAL THEORY IT IS BASED ON. ALSO, YOU MUST PASS THIS LIMITATION OF LIABILITY ON WHENEVER YOU DISTRIBUTE THE WORK OR DERIVATIVE WORKS.\n",
    " \n",
    "8. That if you sue anyone over patents that you think may apply to the Work or anyone's use of the Work, your license to the Work ends automatically.\n",
    " \n",
    "9. That your rights under the License end automatically if you breach it in any way.\n",
    " \n",
    "10. Uber Technologies, Inc. reserves all rights not expressly granted to you in this License.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_unit_cube(x, lb, ub):\n",
    "    \"\"\"Project to [0, 1]^d from hypercube with bounds lb and ub\"\"\"\n",
    "    assert np.all(lb < ub) and lb.ndim == 1 and ub.ndim == 1 and x.ndim == 2\n",
    "    xx = (x - lb) / (ub - lb)\n",
    "    return xx\n",
    "\n",
    "\n",
    "def from_unit_cube(x, lb, ub):\n",
    "    \"\"\"Project from [0, 1]^d to hypercube with bounds lb and ub\"\"\"\n",
    "    assert np.all(lb < ub) and lb.ndim == 1 and ub.ndim == 1 and x.ndim == 2\n",
    "    xx = x * (ub - lb) + lb\n",
    "    return xx\n",
    "\n",
    "\n",
    "def latin_hypercube(n_pts, dim):\n",
    "    \"\"\"Basic Latin hypercube implementation with center perturbation.\"\"\"\n",
    "    X = np.zeros((n_pts, dim))\n",
    "    centers = (1.0 + 2.0 * np.arange(0.0, n_pts)) / float(2 * n_pts)\n",
    "    for i in range(dim):  # Shuffle the center locataions for each dimension.\n",
    "        X[:, i] = centers[np.random.permutation(n_pts)]\n",
    "\n",
    "    # Add some perturbations within each box\n",
    "    pert = np.random.uniform(-1.0, 1.0, (n_pts, dim)) / float(2 * n_pts)\n",
    "    X += pert\n",
    "    return X\n",
    "\n",
    "\n",
    "class GP(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, lengthscale_constraint, outputscale_constraint, ard_dims):\n",
    "        super(GP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.ard_dims = ard_dims\n",
    "        self.mean_module = ConstantMean()\n",
    "        base_kernel = MaternKernel(lengthscale_constraint=lengthscale_constraint, ard_num_dims=ard_dims, nu=2.5)\n",
    "        self.covar_module = ScaleKernel(base_kernel, outputscale_constraint=outputscale_constraint)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "def train_gp(train_x, train_y, use_ard, num_steps, hypers={}):\n",
    "    \"\"\"Fit a GP model where train_x is in [0, 1]^d and train_y is standardized.\"\"\"\n",
    "    assert train_x.ndim == 2\n",
    "    assert train_y.ndim == 1\n",
    "    assert train_x.shape[0] == train_y.shape[0]\n",
    "\n",
    "    # Create hyper parameter bounds\n",
    "    noise_constraint = Interval(5e-4, 0.2)\n",
    "    if use_ard:\n",
    "        lengthscale_constraint = Interval(0.005, 2.0)\n",
    "    else:\n",
    "        lengthscale_constraint = Interval(0.005, math.sqrt(train_x.shape[1]))  # [0.005, sqrt(dim)]\n",
    "    outputscale_constraint = Interval(0.05, 20.0)\n",
    "\n",
    "    # Create models\n",
    "    with gpytorch.settings.cholesky_jitter(float=1e-1):\n",
    "        likelihood = GaussianLikelihood(noise_constraint=noise_constraint).to(device=train_x.device, dtype=train_y.dtype)\n",
    "        ard_dims = train_x.shape[1] if use_ard else None\n",
    "        model = GP(\n",
    "            train_x=train_x,\n",
    "            train_y=train_y,\n",
    "            likelihood=likelihood,\n",
    "            lengthscale_constraint=lengthscale_constraint,\n",
    "            outputscale_constraint=outputscale_constraint,\n",
    "            ard_dims=ard_dims,\n",
    "        ).to(device=train_x.device, dtype=train_x.dtype)\n",
    "\n",
    "        # Find optimal model hyperparameters\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        # \"Loss\" for GPs - the marginal log likelihood\n",
    "        mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        # Initialize model hypers\n",
    "        if hypers:\n",
    "            model.load_state_dict(hypers)\n",
    "        else:\n",
    "            hypers = {}\n",
    "            hypers[\"covar_module.outputscale\"] = 1.0\n",
    "            hypers[\"covar_module.base_kernel.lengthscale\"] = 0.5\n",
    "            hypers[\"likelihood.noise\"] = 0.005\n",
    "            model.initialize(**hypers)\n",
    "\n",
    "        # Use the adam optimizer\n",
    "        optimizer = torch.optim.Adam([{\"params\": model.parameters()}], lr=0.1)\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = -mll(output, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Switch to eval mode\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class Turbo1:\n",
    "    \"\"\"The TuRBO-1 algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : function handle\n",
    "    lb : Lower variable bounds, numpy.array, shape (d,).\n",
    "    ub : Upper variable bounds, numpy.array, shape (d,).\n",
    "    n_init : Number of initial points (2*dim is recommended), int.\n",
    "    max_evals : Total evaluation budget, int.\n",
    "    batch_size : Number of points in each batch, int.\n",
    "    verbose : If you want to print information about the optimization progress, bool.\n",
    "    use_ard : If you want to use ARD for the GP kernel.\n",
    "    max_cholesky_size : Largest number of training points where we use Cholesky, int\n",
    "    n_training_steps : Number of training steps for learning the GP hypers, int\n",
    "    min_cuda : We use float64 on the CPU if we have this or fewer datapoints\n",
    "    device : Device to use for GP fitting (\"cpu\" or \"cuda\")\n",
    "    dtype : Dtype to use for GP fitting (\"float32\" or \"float64\")\n",
    "\n",
    "    Example usage:\n",
    "        turbo1 = Turbo1(f=f, lb=lb, ub=ub, n_init=n_init, max_evals=max_evals)\n",
    "        turbo1.optimize()  # Run optimization\n",
    "        X, fX = turbo1.X, turbo1.fX  # Evaluated points\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        f,\n",
    "        lb,\n",
    "        ub,\n",
    "        n_init,\n",
    "        max_evals,\n",
    "        batch_size=1,\n",
    "        verbose=True,\n",
    "        use_ard=True,\n",
    "        max_cholesky_size=2000,\n",
    "        n_training_steps=50,\n",
    "        min_cuda=256,\n",
    "        device=\"cpu\",\n",
    "        dtype=\"float32\",\n",
    "    ):\n",
    "\n",
    "        # Very basic input checks\n",
    "        assert lb.ndim == 1 and ub.ndim == 1\n",
    "        assert len(lb) == len(ub)\n",
    "        assert np.all(ub > lb)\n",
    "        assert max_evals > 0 and isinstance(max_evals, int)\n",
    "        assert n_init > 0 and isinstance(n_init, int)\n",
    "        assert batch_size > 0 and isinstance(batch_size, int)\n",
    "        assert isinstance(verbose, bool) and isinstance(use_ard, bool)\n",
    "        assert max_cholesky_size >= 0 and isinstance(batch_size, int)\n",
    "        assert n_training_steps >= 30 and isinstance(n_training_steps, int)\n",
    "        assert max_evals > n_init and max_evals > batch_size\n",
    "        assert device == \"cpu\" or device == \"cuda\"\n",
    "        assert dtype == \"float32\" or dtype == \"float64\"\n",
    "        if device == \"cuda\":\n",
    "            assert torch.cuda.is_available(), \"can't use cuda if it's not available\"\n",
    "\n",
    "        # Save function information\n",
    "        self.f = f\n",
    "        self.dim = len(lb)\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "\n",
    "        # Settings\n",
    "        self.n_init = n_init\n",
    "        self.max_evals = max_evals\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.use_ard = use_ard\n",
    "        self.max_cholesky_size = max_cholesky_size\n",
    "        self.n_training_steps = n_training_steps\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.mean = np.zeros((0, 1))\n",
    "        self.signal_var = np.zeros((0, 1))\n",
    "        self.noise_var = np.zeros((0, 1))\n",
    "        self.lengthscales = np.zeros((0, self.dim)) if self.use_ard else np.zeros((0, 1))\n",
    "\n",
    "        # Tolerances and counters\n",
    "        self.n_cand = min(100 * self.dim, 5000)\n",
    "        self.failtol = np.ceil(np.max([4.0 / batch_size, self.dim / batch_size]))\n",
    "        self.succtol = 3\n",
    "        self.n_evals = 0\n",
    "\n",
    "        # Trust region sizes\n",
    "        self.length_min = 0.5 ** 10\n",
    "        self.length_max = 1.6\n",
    "        self.length_init = 0.8\n",
    "\n",
    "        # Save the full history\n",
    "        self.X = np.zeros((0, self.dim))\n",
    "        self.fX = np.zeros((0, 1))\n",
    "\n",
    "        # Device and dtype for GPyTorch\n",
    "        self.min_cuda = min_cuda\n",
    "        self.dtype = torch.float32 if dtype == \"float32\" else torch.float64\n",
    "        self.device = torch.device(\"cuda\") if device == \"cuda\" else torch.device(\"cpu\")\n",
    "        if self.verbose:\n",
    "            print(\"Using dtype = %s \\nUsing device = %s\" % (self.dtype, self.device))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        self.failcount = None\n",
    "        self.succcount = None\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._restart()\n",
    "\n",
    "\n",
    "\n",
    "    def _restart(self):\n",
    "        self._X = []\n",
    "        self._fX = []\n",
    "        if self.failcount is None:\n",
    "            self.failcount = 0\n",
    "        if self.succcount is None:\n",
    "            self.succcount = 0\n",
    "        self.length = self.length_init\n",
    "\n",
    "    def _adjust_length(self, fX_next):\n",
    "        if np.min(fX_next) < np.min(self._fX) - 1e-3 * math.fabs(np.min(self._fX)):\n",
    "            self.succcount += 1\n",
    "            self.failcount = 0\n",
    "        else:\n",
    "            self.succcount = 0\n",
    "            self.failcount += 1\n",
    "\n",
    "        if self.succcount == self.succtol:  # Expand trust region\n",
    "            self.length = min([2.0 * self.length, self.length_max])\n",
    "            self.succcount = 0\n",
    "        elif self.failcount == self.failtol:  # Shrink trust region\n",
    "            self.length /= 2.0\n",
    "            self.failcount = 0\n",
    "\n",
    "    def _create_candidates(self, X, fX, length, n_training_steps, hypers):\n",
    "        \"\"\"Generate candidates assuming X has been scaled to [0,1]^d.\"\"\"\n",
    "        # Pick the center as the point with the smallest function values\n",
    "        # NOTE: This may not be robust to noise, in which case the posterior mean of the GP can be used instead\n",
    "        assert X.min() >= 0.0 and X.max() <= 1.0\n",
    "\n",
    "        # Standardize function values.\n",
    "        mu, sigma = np.median(fX), fX.std()\n",
    "        sigma = 1.0 if sigma < 1e-6 else sigma\n",
    "        fX = (deepcopy(fX) - mu) / sigma\n",
    "\n",
    "        # Figure out what device we are running on\n",
    "        if len(X) < self.min_cuda:\n",
    "            device, dtype = torch.device(\"cpu\"), torch.float64\n",
    "        else:\n",
    "            device, dtype = self.device, self.dtype\n",
    "\n",
    "        # We use CG + Lanczos for training if we have enough data\n",
    "        with gpytorch.settings.max_cholesky_size(self.max_cholesky_size) and gpytorch.settings.cholesky_jitter(float=1e-1):\n",
    "            X_torch = torch.tensor(X).to(device=device, dtype=dtype)\n",
    "            y_torch = torch.tensor(fX).to(device=device, dtype=dtype)\n",
    "            gp = train_gp(\n",
    "                train_x=X_torch, train_y=y_torch, use_ard=self.use_ard, num_steps=n_training_steps, hypers=hypers\n",
    "            )\n",
    "\n",
    "            # Save state dict\n",
    "            hypers = gp.state_dict()\n",
    "\n",
    "        # Create the trust region boundaries\n",
    "        x_center = X[fX.argmin().item(), :][None, :]\n",
    "        weights = gp.covar_module.base_kernel.lengthscale.cpu().detach().numpy().ravel()\n",
    "        weights = weights / weights.mean()  # This will make the next line more stable\n",
    "        weights = weights / np.prod(np.power(weights, 1.0 / len(weights)))  # We now have weights.prod() = 1\n",
    "        lb = np.clip(x_center - weights * length / 2.0, 0.0, 1.0)\n",
    "        ub = np.clip(x_center + weights * length / 2.0, 0.0, 1.0)\n",
    "\n",
    "        # Draw a Sobolev sequence in [lb, ub]\n",
    "        seed = np.random.randint(int(1e6))\n",
    "        sobol = SobolEngine(self.dim, scramble=True, seed=seed)\n",
    "        pert = sobol.draw(self.n_cand).to(dtype=dtype, device=device).cpu().detach().numpy()\n",
    "        pert = lb + (ub - lb) * pert\n",
    "\n",
    "        # Create a perturbation mask\n",
    "        prob_perturb = min(20.0 / self.dim, 1.0)\n",
    "        mask = np.random.rand(self.n_cand, self.dim) <= prob_perturb\n",
    "        ind = np.where(np.sum(mask, axis=1) == 0)[0]\n",
    "        mask[ind, np.random.randint(0, self.dim - 1, size=len(ind))] = 1\n",
    "\n",
    "        # Create candidate points\n",
    "        X_cand = x_center.copy() * np.ones((self.n_cand, self.dim))\n",
    "        X_cand[mask] = pert[mask]\n",
    "\n",
    "        # Figure out what device we are running on\n",
    "        if len(X_cand) < self.min_cuda:\n",
    "            device, dtype = torch.device(\"cpu\"), torch.float64\n",
    "        else:\n",
    "            device, dtype = self.device, self.dtype\n",
    "\n",
    "        # We may have to move the GP to a new device\n",
    "        gp = gp.to(dtype=dtype, device=device)\n",
    "\n",
    "        # We use Lanczos for sampling if we have enough data\n",
    "        with torch.no_grad(), gpytorch.settings.max_cholesky_size(self.max_cholesky_size):\n",
    "            X_cand_torch = torch.tensor(X_cand).to(device=device, dtype=dtype)\n",
    "            y_cand = gp.likelihood(gp(X_cand_torch)).sample(torch.Size([self.batch_size])).t().cpu().detach().numpy()\n",
    "\n",
    "        # Remove the torch variables\n",
    "        del X_torch, y_torch, X_cand_torch, gp\n",
    "\n",
    "        # De-standardize the sampled values\n",
    "        y_cand = mu + sigma * y_cand\n",
    "\n",
    "        return X_cand, y_cand, hypers\n",
    "\n",
    "    def _select_candidates(self, X_cand, y_cand):\n",
    "        \"\"\"Select candidates.\"\"\"\n",
    "        X_next = np.ones((self.batch_size, self.dim))\n",
    "        for i in range(self.batch_size):\n",
    "            # Pick the best point and make sure we never pick it again\n",
    "            indbest = np.argmin(y_cand[:, i])\n",
    "            X_next[i, :] = deepcopy(X_cand[indbest, :])\n",
    "            y_cand[indbest, :] = np.inf\n",
    "        return X_next\n",
    "\n",
    "    def optimize(self, X_init, fX_init):\n",
    "        \"\"\"Run the full optimization process.\"\"\"\n",
    "        # Initialize parameters\n",
    "        self._restart()\n",
    "        fX_init = fX_init.reshape(-1, 1)\n",
    "        \n",
    "        \n",
    "        # Update budget and set as initial data for this TR\n",
    "        # self.n_evals += self.n_init\n",
    "        ### reset how many iterations can run YZ\n",
    "        self.n_init = len(fX_init)\n",
    "        self.n_evals = self.n_init\n",
    "        self._X = deepcopy(X_init)\n",
    "        self._fX = deepcopy(fX_init)\n",
    "\n",
    "        n_evals, fbest = self.n_evals, self._fX.min()\n",
    "        \n",
    "        # Append data to the global history\n",
    "        self.X = np.vstack((self.X, deepcopy(X_init)))\n",
    "        self.fX = np.vstack((self.fX, deepcopy(fX_init)))\n",
    "\n",
    "        if self.verbose:\n",
    "            fbest = self._fX.min()\n",
    "            print(f\"Starting from fbest = {fbest:.4}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # Thompson sample to get next suggestions\n",
    "        while self.n_evals < self.max_evals and self.length >= self.length_min:\n",
    "            # Warp inputs\n",
    "            X = to_unit_cube(deepcopy(self._X), self.lb, self.ub)\n",
    "\n",
    "            # Standardize values\n",
    "            fX = deepcopy(self._fX).ravel()\n",
    "\n",
    "            # Create th next batch\n",
    "            X_cand, y_cand, _ = self._create_candidates(\n",
    "                X, fX, length=self.length, n_training_steps=self.n_training_steps, hypers={}\n",
    "            )\n",
    "            X_next = self._select_candidates(X_cand, y_cand)\n",
    "\n",
    "            # Undo the warping\n",
    "            X_next = from_unit_cube(X_next, self.lb, self.ub)\n",
    "\n",
    "            # Evaluate batch\n",
    "            fX_next = np.array([[self.f(x)] for x in X_next])\n",
    "\n",
    "            # Update trust region\n",
    "            self._adjust_length(fX_next)\n",
    "\n",
    "            # Update budget and append data\n",
    "            self.n_evals += self.batch_size\n",
    "            self._X = np.vstack((self._X, X_next))\n",
    "            self._fX = np.vstack((self._fX, fX_next))\n",
    "\n",
    "            if self.verbose and fX_next.min() < self.fX.min():\n",
    "                n_evals, fbest = self.n_evals, fX_next.min()\n",
    "                print(f\"{n_evals}) New best: {fbest:.4}\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            # Append data to the global history\n",
    "            self.X = np.vstack((self.X, deepcopy(X_next)))\n",
    "            self.fX = np.vstack((self.fX, deepcopy(fX_next)))\n",
    "\n",
    "                \n",
    "                \n",
    "    def run_steps(self, step=1):\n",
    "        \"\"\"Run the full optimization process.\"\"\"\n",
    "        while self.n_evals < self.max_evals:\n",
    "            if len(self._fX) > 0 and self.verbose:\n",
    "                n_evals, fbest = self.n_evals, self._fX.min()\n",
    "                print(f\"{n_evals}) Restarting with fbest = {fbest:.4}\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            # Initialize parameters\n",
    "            self._restart()\n",
    "\n",
    "            # Generate and evalute initial design points\n",
    "            X_init = latin_hypercube(self.n_init, self.dim)\n",
    "            X_init = from_unit_cube(X_init, self.lb, self.ub)\n",
    "            fX_init = np.array([[self.f(x)] for x in X_init])\n",
    "\n",
    "            # Update budget and set as initial data for this TR\n",
    "            self.n_evals += self.n_init\n",
    "            self._X = deepcopy(X_init)\n",
    "            self._fX = deepcopy(fX_init)\n",
    "\n",
    "            # Append data to the global history\n",
    "            self.X = np.vstack((self.X, deepcopy(X_init)))\n",
    "            self.fX = np.vstack((self.fX, deepcopy(fX_init)))\n",
    "\n",
    "            if self.verbose:\n",
    "                fbest = self._fX.min()\n",
    "                print(f\"Starting from fbest = {fbest:.4}\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            # Thompson sample to get next suggestions\n",
    "            while self.n_evals < self.max_evals and self.length >= self.length_min:\n",
    "                # Warp inputs\n",
    "                X = to_unit_cube(deepcopy(self._X), self.lb, self.ub)\n",
    "\n",
    "                # Standardize values\n",
    "                fX = deepcopy(self._fX).ravel()\n",
    "\n",
    "                # Create th next batch\n",
    "                X_cand, y_cand, _ = self._create_candidates(\n",
    "                    X, fX, length=self.length, n_training_steps=self.n_training_steps, hypers={}\n",
    "                )\n",
    "                X_next = self._select_candidates(X_cand, y_cand)\n",
    "\n",
    "                # Undo the warping\n",
    "                X_next = from_unit_cube(X_next, self.lb, self.ub)\n",
    "\n",
    "                # Evaluate batch\n",
    "                fX_next = np.array([[self.f(x)] for x in X_next])\n",
    "\n",
    "                # Update trust region\n",
    "                self._adjust_length(fX_next)\n",
    "\n",
    "                # Update budget and append data\n",
    "                self.n_evals += self.batch_size\n",
    "                self._X = np.vstack((self._X, X_next))\n",
    "                self._fX = np.vstack((self._fX, fX_next))\n",
    "\n",
    "                if self.verbose and fX_next.min() < self.fX.min():\n",
    "                    n_evals, fbest = self.n_evals, fX_next.min()\n",
    "                    print(f\"{n_evals}) New best: {fbest:.4}\")\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "                # Append data to the global history\n",
    "                self.X = np.vstack((self.X, deepcopy(X_next)))\n",
    "                self.fX = np.vstack((self.fX, deepcopy(fX_next)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4QkHZjGgMAeM"
   },
   "outputs": [],
   "source": [
    "class SearchNodeABS:\n",
    "    \"\"\"\n",
    "    This class defines an abstracted node in any search model.\n",
    "    \n",
    "    obj = SearchNodeABS(identifier = None,\n",
    "                X = None,\n",
    "                y = None,\n",
    "                parent = None,\n",
    "                children = [],\n",
    "                possible_moves = [],\n",
    "                is_root = False,\n",
    "                )\n",
    "    Methods: \n",
    "        make_child(moves): create child nodes based on given moves. Must override.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 X = None,\n",
    "                 y = None,\n",
    "                 **kwargs,\n",
    "                ):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.X\n",
    "    \n",
    "    \n",
    "    # the property section is reserved for overriding\n",
    "    @property\n",
    "    def X(self):\n",
    "        return self._X\n",
    "    \n",
    "    @X.setter\n",
    "    def X(self, val):\n",
    "        self._X = val\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "\n",
    "    @y.setter\n",
    "    def y(self, val):\n",
    "        self._y = val        \n",
    "\n",
    "        \n",
    "    def set_parameters(self, **kwargs):\n",
    "        for key, val in kwargs.items():\n",
    "            if getattr(self, key, 'Not Exist') != 'Not Exist':\n",
    "                setattr(self, key, val)\n",
    "        return\n",
    "\n",
    "    \n",
    "    \n",
    "class RealVector(SearchNodeABS):\n",
    "    \"\"\"\n",
    "    This class stores the pair of (X, y). \n",
    "    Further information may be stored in this class, but in current MCDesent we only use (X, y)\n",
    "    \"\"\"\n",
    "    def __init__(self,       \n",
    "                 X = None,\n",
    "                 y = None,\n",
    "                 lb = None,\n",
    "                 ub = None,\n",
    "                 comment = '',\n",
    "                 **kwargs,\n",
    "                ):\n",
    "\n",
    "        # X is a 1D numpy vector in R^n\n",
    "        # y is its value of a function at X     \n",
    "        super().__init__(\n",
    "                        X = X,\n",
    "                        y = y,                   \n",
    "                       )\n",
    "        \n",
    "        # lower/upper bound for the X value\n",
    "        # can be array or a number\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.comment = comment\n",
    "        self.set_parameters(**kwargs)\n",
    "        \n",
    "        self.X = self._update_bounds(self.X)    \n",
    "        return\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def X(self):\n",
    "        return self._X\n",
    "\n",
    "    \n",
    "    @X.setter\n",
    "    def X(self, val):\n",
    "        if not isinstance(val, np.ndarray):\n",
    "            if isinstance(val, numbers.Number): val = [val]\n",
    "            try:\n",
    "                val = np.array(val, dtype=float)\n",
    "            except:\n",
    "                raise ValueError('X value should be real number vector [R^n]')\n",
    "        self._X = val\n",
    "    \n",
    "\n",
    "    \n",
    "    def _update_bounds(self, array): \n",
    "        try:\n",
    "            array = np.clip(array, self.lb, self.ub)\n",
    "        except Exception as e:\n",
    "            print(\"Fail to set boundary: \"+str(e))  \n",
    "        return array\n",
    "    \n",
    "\n",
    "        \n",
    "class TreeNode:\n",
    "    def __init__(self, \n",
    "                 stats,\n",
    "                 fn,\n",
    "                 identifier=None,\n",
    "                 is_root=False,\n",
    "                 children = None,\n",
    "                 parent = None,\n",
    "                 lb=None,\n",
    "                 ub=None,                 \n",
    "                 local_opt = 'stp',\n",
    "                 uct_alpha = 0.0,\n",
    "                 uct_explore = 1.0,\n",
    "                 uct_improve = 1.0,\n",
    "                 dims = None,\n",
    "                 **kwargs,\n",
    "                ):\n",
    "        \n",
    "        self.stats = stats   # current stats, typically a RealVector Node Object\n",
    "        self.identifier = identifier\n",
    "        self.is_root = is_root  \n",
    "        self.children = children\n",
    "        if self.children is None:\n",
    "            self.children = []\n",
    "        self.Ndescendats = 0\n",
    "        self.update_parent(parent)\n",
    "\n",
    "            \n",
    "        if is_root: \n",
    "            self.level = 0\n",
    "        else:\n",
    "            try:\n",
    "                self.level = self.parent.level + 1\n",
    "            except:\n",
    "                self.level = -1\n",
    "        \n",
    "\n",
    "        self.fn = fn\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        if self.lb is None: self.lb = self.fn.lb\n",
    "        if self.ub is None: self.ub = self.fn.ub    \n",
    "\n",
    "        self.dims = dims\n",
    "        if self.dims is None:\n",
    "            try:\n",
    "                self.dims = self.fn.dims\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        self._uct = 0\n",
    "        self.visit = 0\n",
    "        self.local_opt = local_opt        \n",
    "        self.uct_alpha = uct_alpha\n",
    "        self.uct_explore = uct_explore\n",
    "        self.uct_improve = uct_improve\n",
    "        self.stats_pre = self.stats   # stats before last optimization run\n",
    "        self.is_blocked = False\n",
    "        \n",
    "        \n",
    "        # settings for use gpytorch to guess \n",
    "        self.min_gp = 20 # minimum number of samples to guess\n",
    "        self.min_cuda = 256 # minimum number of samples to put to GPU\n",
    "        self.max_cholesky_size = 4000 # max num of samples to use Cholesky, int\n",
    "        self.gp_increment = 20\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.dtype = torch.float64\n",
    "        self.curr_device = None\n",
    "        self.curr_dtype = None\n",
    "        self.use_ard = True   \n",
    "        self.GP_trainstep=50\n",
    "        self.GP_Nsample = -1\n",
    "        self.uncert_ratio_limit = 2.0 # if the uncertainty of a guess is larger than the max uncertainty of saved sample times this ratio, then get ground truth\n",
    "        self.saved_y_X = []\n",
    "        self.saved_y_uncert = None \n",
    "        self.saved_marker = [] # some comments on the ground truth value\n",
    "        self.gp = None\n",
    "        self.turbo_length = None\n",
    "\n",
    "\n",
    "        # normalization of data \n",
    "        self.X_scaler = preprocessing.MinMaxScaler(clip=True)\n",
    "        self.y_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "        try:\n",
    "            _lb = self.fn.lb\n",
    "            _ub = self.fn.ub\n",
    "            _t = np.vstack((_lb, _ub))\n",
    "            self.X_scaler.fit(_t)\n",
    "        except Exception as e:\n",
    "            print('Failed at transforming Xscaler : %s'%(str(e)))\n",
    "            pass\n",
    "\n",
    "\n",
    "        self.stp_alpha_init = 1.0 # depreciated\n",
    "        self.stp_alpha = self.stp_alpha_init\n",
    "        self.stp_max_same_direction_check = 5\n",
    "        self.stp_cumulate_improv = []\n",
    "        self.stp_groundtruth_check = 0\n",
    "        self.stp_consecutive_guess = 0\n",
    "        self.max_stp_consecutive_guess = 20\n",
    "\n",
    "\n",
    "        self.dy_recent = []  # recent improvements\n",
    "        self.stats_0 = self.stats  # init stats\n",
    "        self.y_0 = None \n",
    "        self.y_best = None\n",
    "\n",
    "\n",
    "        if self.stats is not None:\n",
    "            try:\n",
    "                self.y_0 = self.cal_one(self.stats)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        self.y_best = self.y_0  # best found in this node and all children\n",
    "        self.set_parameters(**kwargs)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def set_parameters(self, **kwargs):\n",
    "        for key, val in kwargs.items():\n",
    "            if getattr(self, key, 'Not Exist') != 'Not Exist':\n",
    "                setattr(self, key, val)\n",
    "        return    \n",
    "    \n",
    "    def update_parent(self, parent):\n",
    "        self.parent = parent\n",
    "        if (self.parent is not None) and (self not in self.parent.children):\n",
    "            self.parent.children.append(self)\n",
    "\n",
    "\n",
    "    def visit_once(self,):\n",
    "        self.visit += 1\n",
    "        \n",
    "    def reset_visit(self,):\n",
    "        self.visit = 0    \n",
    "    \n",
    "    def y_upward(self, y, dy=None):\n",
    "        # dy is the improvement from computed leaf node\n",
    "        # self.visit_once()\n",
    "        if self.y_0 is None:\n",
    "            self.y_0 = y\n",
    "\n",
    "        if self.y_best is None: \n",
    "            self.y_best = y\n",
    "            dy = 0.0\n",
    "        else:\n",
    "            if self.y_best > y:\n",
    "                if dy is None: dy = self.y_best - y\n",
    "                self.y_best = y\n",
    "                print(' y_best improved for node %s' % self.identifier)\n",
    "            else:\n",
    "                if dy is None: dy = 0.0\n",
    "                pass\n",
    "        self.dy_recent.insert(0, dy)\n",
    "        \n",
    "        if self.parent is not None:\n",
    "            self.parent.y_upward(self.y_best, dy)\n",
    "        return\n",
    "\n",
    "\n",
    "    def update_save(self, _y_X, update_parent=True, marker=None):\n",
    "        try:\n",
    "            self.saved_y_X = np.vstack((self.saved_y_X, _y_X))        \n",
    "        except:\n",
    "            self.saved_y_X = copy.deepcopy(_y_X)\n",
    "            self.saved_y_X = self.saved_y_X.reshape(1, -1)\n",
    "        self.saved_marker.append(marker)\n",
    "        if update_parent and self.parent is not None:\n",
    "            self.parent.update_save(_y_X, update_parent=update_parent, marker=marker)\n",
    "        return\n",
    "\n",
    "\n",
    "    def cal_one(self, _x):\n",
    "\n",
    "        if isinstance(_x, SearchNodeABS):\n",
    "            if _x.y is not None: \n",
    "                return _x.y # already calculated\n",
    "            x = copy.deepcopy(_x.X)\n",
    "        else:\n",
    "            x = copy.deepcopy(_x)\n",
    "        \n",
    "        assert isinstance(x, np.ndarray) or isinstance(x, numbers.Number)\n",
    "\n",
    "        y = self.get_groundtruth(x, marker='initial_node')\n",
    "\n",
    "        if isinstance(y, np.ndarray) : \n",
    "            if y.dims == 1: y = y.item()\n",
    "        if isinstance(_x, SearchNodeABS): \n",
    "            _x.y = y\n",
    "            \n",
    "        return y\n",
    "\n",
    "    def get_groundtruth(self, x, marker='Turbo', ifAddCumulate=False):\n",
    "        # Get the ground truth and add to self.saved_y_X\n",
    "        _X = None # deepcopy X to save in this obj \n",
    "        _y = None # deepcopy y to save in this obj\n",
    "        \n",
    "        if isinstance(x, SearchNodeABS):\n",
    "            _X = copy.deepcopy(x.X)\n",
    "        elif isinstance(x, np.ndarray):\n",
    "            _X = copy.deepcopy(x)\n",
    "        \n",
    "        if _X.ndim == 2 and len(_X) == 1:\n",
    "            _X  = _X.reshape(-1)\n",
    "\n",
    "\n",
    "        _X = np.clip(_X, self.fn.lb, self.fn.ub)\n",
    "        _y = self.fn(_X)\n",
    "        \n",
    "#         if isinstance(_y, numbers.Number):\n",
    "#             _y = np.array([_y])\n",
    "        # case for functions:\n",
    "        # case 1: x = [1 x n] , y = [1]        \n",
    "        # case 2: x = [1 x n] , y = [n]        \n",
    "        # case 3: x = [m x n] , y = [m]\n",
    "        if _X.ndim == 1:  \n",
    "            try:\n",
    "                # case 1: x = [1 x n] , y = [1]\n",
    "                if len(_X) != len(_y) : \n",
    "                    _X = _X.reshape(-1)\n",
    "                    _y = _y.reshape(-1)\n",
    "                else: # case 2: x = [1 x n] , y = [n] \n",
    "                    _X = _X.reshape(-1, 1)\n",
    "                    _y = _y.reshape(-1, 1)\n",
    "            except:\n",
    "                pass\n",
    "        else: \n",
    "            # case 3: x = [m x n] , y = [m]\n",
    "            _y = _y.reshape(-1, 1)\n",
    "            \n",
    "        _y_X = np.hstack((_y, _X))\n",
    "\n",
    "        self.update_save(_y_X, update_parent=True, marker=marker+\"@\"+self.identifier)            \n",
    "        if isinstance(x, SearchNodeABS): x.y = copy.deepcopy(_y)\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            if self.y_best is None:\n",
    "                if self.dims is None:\n",
    "                        self.dims = _X.shape[-1]\n",
    "            elif _y < self.y_best:\n",
    "                x2_vector = self.stats.__class__(_X, y=_y, parent=None, \n",
    "                                    lb=self.stats.lb, ub=self.stats.ub,\n",
    "                                    )\n",
    "                self.stats = x2_vector\n",
    "                # self.y_best = _y # this update is included in y_upward\n",
    "                if marker is not None:\n",
    "                    print('    Find better value by : %s ; value = %.6f' %(marker,_y))\n",
    "\n",
    "            if ifAddCumulate:\n",
    "                stp_improv = self.y_best - _y if self.y_best > _y else 0.0\n",
    "                self.stp_cumulate_improv.append(stp_improv)\n",
    "        except:\n",
    "            print('Fail at updating y_best at groundtruth, ', _y, self.y_best)\n",
    "\n",
    "        self.y_upward(_y)\n",
    "        return _y    \n",
    "\n",
    "\n",
    "\n",
    "    def _choose_gp_node(self, node=None):\n",
    "        if node is None:\n",
    "            node = self\n",
    "        while len(node.saved_y_X) < node.min_gp:\n",
    "            if node.is_root:\n",
    "                return node\n",
    "            else:\n",
    "                node = node.parent\n",
    "        return node\n",
    "\n",
    "    \n",
    "    def _create_gp(self, _x=None):\n",
    "        # Input: if use points close to _x for creating GP\n",
    "        # Return: if successfully create a GP\n",
    "\n",
    "        if len(self.saved_y_X) < self.min_gp: # if no enough samples, and is not a root node, do nothing\n",
    "            return False\n",
    "        \n",
    "        if not self.is_root :\n",
    "            device, dtype = torch.device(\"cpu\"), torch.float64\n",
    "        else:\n",
    "            if len(self.saved_y_X) < self.min_cuda:\n",
    "                device, dtype = torch.device(\"cpu\"), torch.float64\n",
    "            else:\n",
    "                device, dtype = self.device, self.dtype  \n",
    "\n",
    "        if self.gp is not None and self.GP_Nsample > 0:\n",
    "            if len(self.saved_y_X) <= self.GP_Nsample + self.gp_increment:\n",
    "                return True\n",
    "            else:\n",
    "                self._delete_gp()\n",
    "\n",
    "        if _x is None:\n",
    "            _use_sample = np.arange(len(self.saved_y_X))\n",
    "        else:\n",
    "            if _x.ndim == 1: _x = _x.reshape(1, -1)\n",
    "\n",
    "\n",
    "            if len(self.saved_y_X) < self.min_cuda:\n",
    "                _use_sample = np.arange(len(self.saved_y_X))\n",
    "            else:\n",
    "                _N = self.min_cuda\n",
    "\n",
    "                _x_mean = np.mean(_x, axis=0)\n",
    "                _dist = np.linalg.norm(self.saved_y_X[:, 1:]-_x_mean, axis=1)\n",
    "\n",
    "                _use_sample = np.argsort(_dist)[:_N]     \n",
    "\n",
    "        _X_scaled = self.X_scaler.transform(self.saved_y_X[_use_sample, 1: ])\n",
    "        _y_scaled = self.y_scaler.fit_transform(self.saved_y_X[_use_sample, 0:1])\n",
    "        \n",
    "        with gpytorch.settings.max_cholesky_size(self.max_cholesky_size):\n",
    "            X_torch = torch.tensor(_X_scaled).to(device=device, dtype=dtype)\n",
    "            y_torch = torch.tensor(_y_scaled).to(device=device, dtype=dtype).reshape(-1)\n",
    "            try:\n",
    "                del self.gp\n",
    "                self.gp = None\n",
    "            except:\n",
    "                pass\n",
    "            self.gp = train_gp(\n",
    "                train_x=X_torch, train_y=y_torch, \n",
    "                use_ard=self.use_ard, \n",
    "                num_steps=self.GP_trainstep, \n",
    "                hypers={},\n",
    "            )\n",
    "            \n",
    "            self.saved_y_uncert = np.sqrt(self.gp(X_torch).variance.cpu().detach().numpy().reshape(-1))\n",
    "            self.saved_y_uncert *= np.sqrt(self.y_scaler.var_)\n",
    "            self.saved_y_uncert = self.saved_y_uncert.reshape(-1)\n",
    "            del X_torch, y_torch\n",
    "\n",
    "            self.curr_device = device\n",
    "            self.curr_dtype = dtype\n",
    "            self.GP_Nsample = len(self.saved_y_X)\n",
    "        return True\n",
    "    \n",
    "    \n",
    "\n",
    "    def _delete_gp(self):\n",
    "        if self.gp is not None:\n",
    "            del self.gp\n",
    "            self.GP_Nsample = -1\n",
    "            self.curr_device = None\n",
    "            self.curr_dtype = None\n",
    "            self.gp = None\n",
    "\n",
    "\n",
    "    def guess(self, _x, isScaled=False):\n",
    "        start = time.time()   \n",
    "        isGPSuccess = self._create_gp()\n",
    "        if not isGPSuccess:\n",
    "            return  False, 0.0, 0., 0.\n",
    "\n",
    "        device, dtype = self.curr_device, self.curr_dtype\n",
    "\n",
    "        # scale the sample for prediction\n",
    "        if _x.ndim == 1:\n",
    "            _x = _x.reshape(1, -1)\n",
    "        \n",
    "        _x_scaled = _x\n",
    "        if not isScaled:\n",
    "            _x_scaled = self.X_scaler.transform(_x_scaled)\n",
    "        _x_scaled = np.clip(_x_scaled, 0., 1.)  # XScaler uses MinMaxScale with [0, 1] as output\n",
    "\n",
    "\n",
    "        _x_torch = torch.tensor(_x_scaled).to(device=device, dtype=dtype)\n",
    "        \n",
    "        pred_y_scaled = self.gp(_x_torch).mean.cpu().detach().numpy().reshape(-1, 1)        \n",
    "        uncert_y_scaled = np.sqrt(self.gp(_x_torch).variance.cpu().detach().numpy().reshape(-1)) # use std as uncertainty        \n",
    "        \n",
    "        pred_y = self.y_scaler.inverse_transform(pred_y_scaled).reshape(-1)  # inverse transform\n",
    "        uncert_y = uncert_y_scaled * np.sqrt(self.y_scaler.var_).reshape(-1)  # scale back the std \n",
    "        \n",
    "        del _x_torch\n",
    "        \n",
    "        is_reliable = np.empty_like(uncert_y, dtype=int)\n",
    "        is_reliable[:] = False\n",
    "        is_reliable[uncert_y <= np.max(self.saved_y_uncert) * self.uncert_ratio_limit ] = True\n",
    "\n",
    "        return True, pred_y, is_reliable, uncert_y   \n",
    "\n",
    "\n",
    "#### Descent Optimization\n",
    "    def opt_node(self, max_budget=1, opt=None, params={}, **kwargs):\n",
    "        if opt == 'stp':\n",
    "            method = self._local_stp\n",
    "            self.stp_groundtruth_check = 0\n",
    "            while self.stp_groundtruth_check < max_budget:\n",
    "                method(**kwargs)\n",
    "        elif opt == 'zigzag':  # zigzag is another version of STP with more guesses at local\n",
    "            method = self._local_stp_zigzag\n",
    "            self.stp_groundtruth_check = 0\n",
    "            while self.stp_groundtruth_check < max_budget:\n",
    "                method(**kwargs)\n",
    "        return    \n",
    "\n",
    "\n",
    "    \n",
    "    def _local_stp_zigzag(self, \n",
    "                          iteration = 1,\n",
    "                          cumulate_improve_thres = 5e-2,\n",
    "                          cumulate_improve_count = 10,\n",
    "                          use_alpha = True,\n",
    "                          use_mask = False,\n",
    "                          ):\n",
    "        stats = self.stats\n",
    "        _x_init = copy.deepcopy(stats.X)\n",
    "\n",
    "        # make x as scaled\n",
    "        _x_init = self.X_scaler.transform(_x_init.reshape(1,-1))\n",
    "\n",
    "        n = self.visit if self.visit > 0 else 1\n",
    "    \n",
    "        Ncand = min(_x_init.shape[-1]*500, 100000)\n",
    "\n",
    "        dx_cand_all = self._dx_sample(_x_init, \n",
    "                                      epoch=Ncand,\n",
    "                                      range = self.stp_alpha,\n",
    "                                      use_alpha = use_alpha,\n",
    "                                      use_mask = use_mask,\n",
    "                                     )\n",
    "\n",
    "        node = self._choose_gp_node()\n",
    "\n",
    "        xscale_best_curr = self.X_scaler.transform(self.stats.X.reshape(1,-1))\n",
    "        y_best_curr = copy.deepcopy(self.stats.y)\n",
    "\n",
    "        isSuccGuess, y_cand_all, isreliable_cand, y_cand_uncert_all = node.guess(_x_init + dx_cand_all, isScaled=True)\n",
    "        if isSuccGuess:\n",
    "            idx_cand = np.argmin(y_cand_all)\n",
    "        else:\n",
    "            idx_cand = np.random.randint(Ncand)\n",
    "        dx_cand = dx_cand_all[idx_cand]\n",
    "\n",
    "        x = _x_init\n",
    "        _scale = 1.0\n",
    "        _it = 0\n",
    "\n",
    "        # here defines how much dx * _scale to examin\n",
    "        leftChecked, rightChecked = 0., 0.\n",
    "        left, right = -1., 1.\n",
    "        curr = 0.\n",
    "        shrink_fail = 0\n",
    "\n",
    "        \n",
    "        self.stp_cumulate_improv = []\n",
    "        for i in range(iteration):\n",
    "            succOnce = False\n",
    "            isSuccess_right = False\n",
    "            isSuccess_right, xscale_best_curr, y_best_curr = self._local_stp_try(x, dx_cand, (right-curr) * _scale, xscale_best_curr, y_best_curr)\n",
    "            if isSuccess_right:\n",
    "                succOnce = True\n",
    "                new_t = right\n",
    "                if new_t >= rightChecked:\n",
    "                    right = new_t * 2.\n",
    "                    rightChecked = right\n",
    "                else:\n",
    "                    right = (rightChecked + new_t)/2\n",
    "\n",
    "                leftChecked = curr\n",
    "                left = (curr + new_t)/2\n",
    "                curr = new_t\n",
    "                \n",
    "\n",
    "            isSuccess_left = False\n",
    "            if not isSuccess_right:\n",
    "                isSuccess_left, xscale_best_curr, y_best_curr = self._local_stp_try(x, dx_cand, (left-curr) * _scale, xscale_best_curr, y_best_curr)\n",
    "            if isSuccess_left:\n",
    "                succOnce = True\n",
    "                new_t = left\n",
    "                if new_t <= leftChecked:\n",
    "                    left = new_t * 2.\n",
    "                    leftChecked = left\n",
    "                else:\n",
    "                    left = (leftChecked + new_t)/2\n",
    "                rightChecked = curr # the closest checked position to the right\n",
    "                right = (curr + new_t)/2 # next check of right\n",
    "                curr = new_t\n",
    "                \n",
    "\n",
    "            if not isSuccess_right and not isSuccess_left:\n",
    "                shrink_fail += 1\n",
    "                if shrink_fail == 10:\n",
    "                    break\n",
    "\n",
    "                leftChecked = left\n",
    "                left = (left + curr)/2\n",
    "                \n",
    "                rightChecked = right\n",
    "                right = (right + curr)/2\n",
    "\n",
    "            # exhausted stp search : break \n",
    "            if len(self.stp_cumulate_improv) > cumulate_improve_count:\n",
    "                if sum(self.stp_cumulate_improv[-cumulate_improve_count:]) < abs(self.y_best) * cumulate_improve_thres:\n",
    "                    break\n",
    "\n",
    "        if succOnce :\n",
    "            x_best_curr = self.X_scaler.inverse_transform(xscale_best_curr.reshape(1,-1)).reshape(-1)\n",
    "            if x_best_curr not in self.saved_y_X[:, 1:]:\n",
    "                self.get_groundtruth(x, marker = 'local_stp_zigzag_best_guess')\n",
    "                self.stp_groundtruth_check += 1\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _local_stp(self, \n",
    "                   use_alpha = True,\n",
    "                   use_mask = False,\n",
    "                   **kwargs):\n",
    "        _x_init = copy.deepcopy(self.stats.X)\n",
    "        \n",
    "        # make x as scaled\n",
    "        _x_init = self.X_scaler.transform(_x_init.reshape(1, -1))\n",
    "        n = self.visit if self.visit > 0 else 1\n",
    "    \n",
    "        Ncand = min(_x_init.shape[-1]*50, 100000)\n",
    "        dx_cand_all = self._dx_sample(_x_init, \n",
    "                                     epoch=Ncand,\n",
    "                                     range = self.stp_alpha,\n",
    "                                     use_alpha = use_alpha,\n",
    "                                     use_mask = use_mask,\n",
    "                                     )\n",
    "\n",
    "\n",
    "\n",
    "        node = self._choose_gp_node()\n",
    "\n",
    "        xscale_best_curr = self.X_scaler.transform(self.stats.X.reshape(1,-1))\n",
    "        y_best_curr = self.stats.y\n",
    "        isSuccGuess, y_cand_all, isreliable_cand, y_cand_uncert_all = node.guess(_x_init + dx_cand_all, isScaled=True)\n",
    "        if isSuccGuess:\n",
    "            idx_cand = np.argmin(y_cand_all)\n",
    "        else:\n",
    "            idx_cand = np.random.randint(Ncand)\n",
    "        \n",
    "        dx_cand = dx_cand_all[idx_cand]\n",
    "\n",
    "        x = _x_init\n",
    "\n",
    "        \n",
    "        # try with x + dx\n",
    "        _scale = 1.0\n",
    "        _it = 0\n",
    "\n",
    "        isSuccess_posx = False\n",
    "        isSuccess_negx = False\n",
    "        isSuccess_posx_shrink = False\n",
    "        isSuccess_negx_shrink = False\n",
    "        isSuccess_posx, xscale_best_curr, y_best_curr = self._local_stp_try(x, dx_cand, _scale, xscale_best_curr, y_best_curr )\n",
    "        if isSuccess_posx:  # if  success on +dx\n",
    "            isSuccess_consecutive = True\n",
    "            while isSuccess_consecutive:\n",
    "                _scale *= 2\n",
    "                isSuccess_consecutive, xscale_best_curr, y_best_curr  = self._local_stp_try(x, dx_cand, _scale, xscale_best_curr, y_best_curr )\n",
    "        else:  # try with -dx\n",
    "            isSuccess_negx, xscale_best_curr, y_best_curr  = self._local_stp_try(x, dx_cand, -1*_scale, xscale_best_curr, y_best_curr )\n",
    "            isSuccess_consecutive = isSuccess_negx\n",
    "            while isSuccess_consecutive:\n",
    "                _scale *= 2\n",
    "                isSuccess_consecutive, xscale_best_curr, y_best_curr  = self._local_stp_try(x, dx_cand, -1*_scale, xscale_best_curr, y_best_curr )\n",
    "\n",
    "\n",
    "        if (not isSuccess_posx) and (not isSuccess_negx):\n",
    "            _scale *= 0.5\n",
    "            isSuccess_posx_shrink, xscale_best_curr, y_best_curr  = self._local_stp_try(x, dx_cand, _scale, xscale_best_curr, y_best_curr )\n",
    "            isSuccess_consecutive = isSuccess_posx_shrink\n",
    "            while isSuccess_consecutive:\n",
    "                _scale *= 0.5\n",
    "                isSuccess_consecutive, xscale_best_curr, y_best_curr  = self._local_stp_try(x, dx_cand, _scale, xscale_best_curr, y_best_curr )\n",
    "            if not isSuccess_posx_shrink:\n",
    "                isSuccess_negx_shrink, xscale_best_curr, y_best_curr  = self._local_stp_try(x, dx_cand, -1.*_scale, xscale_best_curr, y_best_curr )\n",
    "                isSuccess_consecutive = isSuccess_negx_shrink\n",
    "                while isSuccess_consecutive:\n",
    "                    _scale *= 0.5\n",
    "                    isSuccess_consecutive, xscale_best_curr, y_best_curr  = self._local_stp_try(x, dx_cand, -1.* _scale, xscale_best_curr, y_best_curr )\n",
    "\n",
    "\n",
    "        succOnce = isSuccess_posx or isSuccess_negx or isSuccess_posx_shrink or isSuccess_negx_shrink\n",
    "        if succOnce :\n",
    "            x_best_curr = self.X_scaler.inverse_transform(xscale_best_curr.reshape(1, -1)).reshape(-1)\n",
    "            if x_best_curr not in self.saved_y_X[:, 1:]:\n",
    "                self.stp_groundtruth_check += 1\n",
    "                self.get_groundtruth(x_best_curr, marker = 'local_stp_best_guess')\n",
    "        return\n",
    "\n",
    "\n",
    "    def _dx_sample(self, \n",
    "                      x = None,\n",
    "                      epoch=10000,\n",
    "                      range = 1.0,\n",
    "                      use_alpha = False,\n",
    "                      use_mask = False,\n",
    "                      seed = None,\n",
    "                    ):\n",
    "        \n",
    "        # return Latin Cubic sampling within [-1., +1.], \n",
    "        # can be scaled by GP CL\n",
    "\n",
    "        if x is None:\n",
    "            x = self.stats.X\n",
    "\n",
    "        start = time.time()\n",
    "        dims = x.shape[-1]\n",
    "\n",
    "        sampler = 0\n",
    "        if seed is None:\n",
    "            sampler = qmc.LatinHypercube(d=dims)\n",
    "        else:\n",
    "            sampler = qmc.LatinHypercube(d=dims, seed=seed)\n",
    "\n",
    "        dx_cand = 2 * sampler.random(n=epoch) - 1.0\n",
    "\n",
    "        if use_alpha:\n",
    "            node = self\n",
    "            while len(node.saved_y_X) < node.min_gp and not node.is_root:\n",
    "                node = node.parent\n",
    "            try:\n",
    "                node._create_gp()\n",
    "                alpha = node.gp.covar_module.base_kernel.lengthscale.cpu().detach().numpy().ravel()\n",
    "                \n",
    "                # rescale to approximate the same length\n",
    "                norm_old = np.linalg.norm(dx_cand, axis=1).reshape(-1, 1)\n",
    "                dx_cand *= alpha \n",
    "                norm_new = np.linalg.norm(dx_cand, axis=1).reshape(-1, 1)\n",
    "                dx_cand *= norm_old/norm_new\n",
    "\n",
    "            except Exception as e:    \n",
    "                print('failed at sampling with CL: ', str(e))\n",
    "                pass\n",
    "            \n",
    "        # add a scale factor \n",
    "        dx_cand *= range\n",
    "\n",
    "        return dx_cand\n",
    "\n",
    "\n",
    "    def _local_stp_try(self, _x, dx, scale, _x_prev=None, _y_prev=None, ):\n",
    "\n",
    "        # _x and dx are scaled by self.X_scaler\n",
    "        start = time.time()\n",
    "        if _x_prev is None:\n",
    "            _x_prev = self.X_scaler.transform(self.stats.X.reshape(1, -1))\n",
    "        if _y_prev is None:\n",
    "            _y_prev = self.stats.y\n",
    "\n",
    "        _x2 = np.clip(_x + scale * dx, 0., 1.)\n",
    "\n",
    "        isSuccGuess = False\n",
    "        if self.stp_consecutive_guess < self.max_stp_consecutive_guess:\n",
    "            node = self._choose_gp_node()\n",
    "            isSuccGuess, _y2, _, _ = node.guess(_x2, isScaled=True)\n",
    "            if isSuccGuess:\n",
    "                self.stp_consecutive_guess += 1\n",
    "            else:\n",
    "                self.stp_consecutive_guess = 0\n",
    "                self.stp_groundtruth_check += 1\n",
    "                _y2 = self.get_groundtruth(self.X_scaler.inverse_transform(_x2.reshape(1, -1)), marker = 'local_stp_try', ifAddCumulate=True,) \n",
    "        else:\n",
    "            self.stp_consecutive_guess = 0\n",
    "            self.stp_groundtruth_check += 1\n",
    "            _y2 = self.get_groundtruth(self.X_scaler.inverse_transform(_x2.reshape(1, -1)), marker = 'local_stp_try', ifAddCumulate=True,)       # after this step, self.y_best is updated  \n",
    "        is_success = False\n",
    "        \n",
    "        x_to_return = _x_prev\n",
    "        y_to_return = _y_prev\n",
    "        if _y2 < _y_prev:\n",
    "            is_success = True\n",
    "            x_to_return = _x2\n",
    "            y_to_return = _y2\n",
    "        end = time.time()\n",
    "        return is_success, x_to_return, y_to_return\n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "    def uct(self):\n",
    "        self.cal_uct()\n",
    "        return self._uct\n",
    "    \n",
    "\n",
    "    def cal_uct(self, pvisit=None, count_recent=40):\n",
    "        if pvisit is None:\n",
    "            if self.parent is not None:\n",
    "                pvisit = self.parent.visit\n",
    "            else:\n",
    "                pvisit = 1\n",
    "        assert isinstance(pvisit, int)\n",
    "        \n",
    "        if len(self.dy_recent) > 0:\n",
    "            improv = np.sum(self.dy_recent[:count_recent])                \n",
    "        else:\n",
    "            improv = 0.\n",
    "\n",
    "        qv = self.y_best \n",
    "        exploration = np.sqrt(np.log(np.max((pvisit, 1.0))) / np.max((self.visit, 1.0)) )\n",
    "\n",
    "        self._uct = -qv \\\n",
    "                    + self.uct_improve * improv \\\n",
    "                    + self.uct_alpha * (self.y_0 - self.y_best) / np.max((self.visit, 1.0)) \\\n",
    "                    + self.uct_explore * exploration\n",
    "        return    \n",
    "    \n",
    "    \n",
    "    def create_kids_turbo(self, iteration=40, use_node = None, save_length=True):\n",
    "        if use_node is None: use_node = self\n",
    "        N_init = len(use_node.saved_y_X)\n",
    "        \n",
    "        y_best_prev = copy.deepcopy(self.y_best)\n",
    "        with gpytorch.settings.cholesky_jitter(float=1e-1, double=1e-2):\n",
    "            model = Turbo1(f=self.get_groundtruth, # Handle to objective function  \n",
    "                        lb=self.lb,  # Numpy array specifying lower bounds\n",
    "                        ub=self.ub,  # Numpy array specifying upper bounds\n",
    "                        n_init=N_init,  # Number of initial bounds from an Latin hypercube design\n",
    "                        max_evals =N_init + iteration,  # Maximum number of evaluations\n",
    "                        batch_size=1,  # How large batch size TuRBO uses\n",
    "                        verbose=False,  # Print information from each batch\n",
    "                        use_ard=True,  # Set to true if you want to use ARD for the GP kernel\n",
    "                        max_cholesky_size=2000,  # When we switch from Cholesky to Lanczos\n",
    "                        n_training_steps=50,  # Number of steps of ADAM to learn the hypers\n",
    "                        min_cuda=use_node.min_cuda,  # Run on the CPU for small datasets\n",
    "                        device='cuda',  # \"cpu\" or \"cuda\"\n",
    "                        dtype='float64',  # float64 or float32\n",
    "                        )\n",
    "            if save_length:\n",
    "                if self.turbo_length is not None:\n",
    "                    if self.turbo_length > model.length_min and self.turbo_length < model.length_max :\n",
    "                        model.length_init = self.turbo_length\n",
    "                        try:\n",
    "                            model.succcount = self.turbo_succcount\n",
    "                            model.failcount = self.turbo_failcount\n",
    "                        except:\n",
    "                            model.succcount = 0\n",
    "                            model.failcount = 0\n",
    "            model.optimize(use_node.saved_y_X[:, 1:], use_node.saved_y_X[:, 0])\n",
    "\n",
    "            if save_length:\n",
    "                self.turbo_length = copy.deepcopy(model.length)\n",
    "                self.turbo_succcount = model.succcount\n",
    "                self.turbo_failcount = model.failcount \n",
    "            self.stp_alpha = copy.deepcopy(model.length)\n",
    "\n",
    "            del model\n",
    "        \n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCDesentSearch( test_fn,\n",
    "                    dims = None,\n",
    "                    log = \"./log.dat\", \n",
    "                    max_steps = 100,            # Number of total iterations \n",
    "                    iter_descent = 10,          # Number of allowed function calls by descent in every iteration\n",
    "                    iter_turbo = 20,            # Number of allowed function calls by TuRBO in every iteration\n",
    "                    stp_internal_steps = 10,    # Number of allowed guess calls by descent \n",
    "                    width = 0.1,                # Relative distance scale at descent\n",
    "                    stp_switch_thres = 35000.,  # When if STP wants to switch for more guess\n",
    "                    rad_max_decay = 0.5,        # Max relative distance when create new node\n",
    "                    rad_min_decay = 0.1,        # Min relative distance when create new node\n",
    "                    node_uct_improve = 50.,     # C_d  for node\n",
    "                    node_uct_explore = 1. ,     # C_p  for node\n",
    "                    leaf_improve = 100.,        # C_d'' for leaf\n",
    "                    leaf_explore = 10.,         # C_p'' for leaf\n",
    "                    branch_explore = 1000.0,    # C_d' for branch \n",
    "                    **kwargs,\n",
    "                  ):\n",
    "        \"\"\"\n",
    "        This function defines the entire process of MCDesent search.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            test_fn = fn(dims = dims)\n",
    "        except:\n",
    "            test_fn = fn()\n",
    "            dims = test_fn.dims\n",
    "\n",
    "        print(\"=========================================================\")\n",
    "        print(\"start evaluating function %s in dim %d \"%(fn, dims))\n",
    "        try:\n",
    "            print(\" lb = %f,  ub = %f\"%(test_fn.lb[0], test_fn.ub[0]))\n",
    "        except:\n",
    "            print(\" lb = %f,  ub = %f\"%(test_fn.lb, test_fn.ub))\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # control how many improvements are considered in the history\n",
    "        explore_recent_improve_count = 3*(iter_descent + iter_turbo)  # np.sum(dy_recent[:explore_recent_improve_count])\n",
    "\n",
    "        \n",
    "        test_fn.counter = 0\n",
    "        step = 0\n",
    "        \n",
    "        \n",
    "        sample = np.random.rand(dims)\n",
    "        val = test_fn.lb + (test_fn.ub - test_fn.lb) * sample\n",
    "        vec_root = RealVector(val, \n",
    "                    lb=test_fn.lb, ub=test_fn.ub,\n",
    "                    )\n",
    "        root = TreeNode(vec_root, test_fn, is_root=True, \n",
    "                        identifier = 'root',\n",
    "                        device = torch.device(\"cuda\"),\n",
    "                        min_gp = 64,\n",
    "                        uct_explor = node_uct_explore,\n",
    "                        uct_improv = node_uct_improve,\n",
    "                        )\n",
    "\n",
    "        \n",
    "        for istep in range(max_steps):\n",
    "\n",
    "            # Choose the correct path and leaf\n",
    "            curr_node = root\n",
    "            print('Step %d , curr queries: %d, curr_best: %f'%(istep, len(root.saved_y_X), root.y_best ))\n",
    "            print('Finding path in step %d  ...'%istep)\n",
    "\n",
    "            to_explore_branch = False\n",
    "\n",
    "            while curr_node.children:\n",
    "                to_explore_branch = False\n",
    "\n",
    "                to_explore_node = lambda: None\n",
    "                to_explore_node.identifier = curr_node.identifier + \" @IfExplore\"\n",
    "                to_explore_node.y_best = curr_node.y_best\n",
    "                to_explore_node.dy_recent = []\n",
    "                to_explore_node.uct = 0\n",
    "\n",
    "                qv = []\n",
    "                for k in curr_node.children:\n",
    "                    qv.append(k.y_best)\n",
    "                qv = np.mean(qv)\n",
    "                to_explore_node.uct += -qv\n",
    "\n",
    "                pvisit = 1\n",
    "                try:\n",
    "                    pvisit = curr_node.parent.visit\n",
    "                except:\n",
    "                    pass\n",
    "                to_explore_node.uct += branch_explore * np.sqrt(np.log(np.max((curr_node.visit, 1.0))) / len(curr_node.children) )  # the more children, the less exploration\n",
    "\n",
    "\n",
    "                check_list = [to_explore_node, *curr_node.children]\n",
    "                check_list.sort(key=lambda node: node.uct, reverse=True) \n",
    "                for kid in check_list:\n",
    "                    print('         children y_best/uct/dy_recent:', kid.identifier, kid.y_best, kid.uct, kid.dy_recent[:10], )\n",
    "\n",
    "                if check_list[0] == to_explore_node:\n",
    "                    to_explore_branch = True\n",
    "                    curr_node = curr_node\n",
    "                else:\n",
    "                    curr_node = check_list[0]\n",
    "                    print(\"  Choose: \" , curr_node.identifier, curr_node.y_best, curr_node.uct)\n",
    "\n",
    "\n",
    "                \n",
    "                if to_explore_branch:\n",
    "\n",
    "                    print('      To explore branch node : %s ' % curr_node.identifier )\n",
    "                    # create an exploration kid\n",
    "                    sample = curr_node._dx_sample(epoch=1, use_alpha=False)[0]\n",
    "\n",
    "                    # Use relative distance from boundaries\n",
    "                    # + means towards ub, - means towards lb\n",
    "                    # e.g., +0.2 means to move 20% to ub, and -0.15 means move 15% to lb\n",
    "                    var_dist_p = (rad_max_decay**curr_node.level)     * sample               # random percentage towards boundaries\n",
    "                    min_dist_p = (rad_min_decay**(curr_node.level+1)) * np.sign(var_dist_p)  # minimum percentage towards boundaries\n",
    "\n",
    "                    dist = test_fn.ub - curr_node.stats.X\n",
    "                    dist_neg = curr_node.stats.X - test_fn.lb\n",
    "                    mask = var_dist_p<0\n",
    "                    dist[mask] = dist_neg[mask]  # this is the 100% distance value from x to boundary\n",
    "\n",
    "                    perc_mtx = np.ones_like(var_dist_p)\n",
    "                    final_p =  (perc_mtx-np.abs(min_dist_p)) * var_dist_p + min_dist_p\n",
    "                    var_dist = dist * final_p\n",
    "\n",
    "                    val = curr_node.stats.X + var_dist\n",
    "                    val = np.clip(val, test_fn.lb, test_fn.ub)\n",
    "                    vector = RealVector(val,\n",
    "                                        lb=test_fn.lb, ub=test_fn.ub,\n",
    "                                        )\n",
    "                    node = TreeNode(vector, test_fn, parent=curr_node, \n",
    "                                    identifier = curr_node.identifier + \"_\"  + \"%03d\"%(len(curr_node.children)),\n",
    "                                    uct_improve = node_uct_improve,\n",
    "                                    uct_explore = node_uct_explore,\n",
    "                                    )\n",
    "                    \n",
    "                    var_norm = np.linalg.norm(var_dist, np.inf)\n",
    "                    # min_norm = np.linalg.norm(min_dist, np.inf)\n",
    "                    print('   var norm = %f '%(var_norm))\n",
    "                    # after creating, set it as to opt\n",
    "                    curr_node = node\n",
    "                    break\n",
    "\n",
    "\n",
    "            to_explore_leaf = False\n",
    "            if not to_explore_branch: # if it is set to explore as a branch node, do not check again\n",
    "                if curr_node.visit < 3:\n",
    "                    # to few visits to this node, then always exploit\n",
    "                    print('LEAF node min exploitation not reached:%s' % curr_node.identifier)\n",
    "                    to_explore_leaf = False  \n",
    "                else:\n",
    "                    thres =  leaf_explore * np.sqrt(np.max((curr_node.visit, 1.0)))                  \n",
    "                    Nimprov = explore_recent_improve_count\n",
    "                    if curr_node.y_best == root.y_best:\n",
    "                        Nimprov = int(3*Nimprov)\n",
    "                    improve = np.sum(curr_node.dy_recent[:Nimprov]) \n",
    "                    improve_std = improve * leaf_improve\n",
    "                    find_best = curr_node.y_best\n",
    "                    find_best_std = -1.0 * find_best \n",
    "                    to_explore_leaf = (find_best_std + improve_std) <= thres\n",
    "                    print(' To explore leaf? ... best_std = %f    improve_std = %f   thres = %f'%(find_best_std, improve_std, thres))\n",
    "\n",
    "                    \n",
    "            if to_explore_leaf:\n",
    "                print('      To explore LEAF node : %s ' % curr_node.identifier )\n",
    "                # create an inherit kid\n",
    "                Ncopy = min(40, iter_descent + iter_turbo, len(curr_node.saved_y_X))\n",
    "                _x_center = copy.deepcopy(curr_node.stats.X)\n",
    "                _dist = np.linalg.norm(_x_center - curr_node.saved_y_X[:, 1:], axis=-1)\n",
    "                _idx_Nclosest = np.argpartition(_dist, Ncopy)\n",
    "\n",
    "\n",
    "                dup_kid = TreeNode(copy.deepcopy(curr_node.stats), test_fn, parent = curr_node,\n",
    "                                    identifier = curr_node.identifier + \"_000\",\n",
    "                                    saved_y_X = copy.deepcopy(curr_node.saved_y_X[_idx_Nclosest[:Ncopy]]),\n",
    "                                    dy_recent = copy.deepcopy(curr_node.dy_recent[-40:]),\n",
    "                                    visit = 1,\n",
    "                                    turbo_length = curr_node.turbo_length,\n",
    "                                    uct_improve = node_uct_improve,\n",
    "                                    )\n",
    "\n",
    "\n",
    "                # create an exploration kid\n",
    "                sample = curr_node._dx_sample(epoch=1, use_alpha=False, use_mask=False)[0]\n",
    "\n",
    "                # Use relative distance from boundaries\n",
    "                # + means towards ub, - means towards lb\n",
    "                # e.g., +0.2 means to move 20% to ub, and -0.15 means move 15% to lb\n",
    "                var_dist_p = (rad_max_decay**curr_node.level)     * sample               # random percentage towards boundaries\n",
    "                min_dist_p = (rad_min_decay**(curr_node.level+1)) * np.sign(var_dist_p)  # minimum percentage towards boundaries\n",
    "\n",
    "\n",
    "                dist = test_fn.ub - curr_node.stats.X\n",
    "                dist_neg = curr_node.stats.X - test_fn.lb\n",
    "                mask = var_dist_p<0\n",
    "                dist[mask] = dist_neg[mask]  # this is the 100% distance value from x to boundary\n",
    "\n",
    "                perc_mtx = np.ones_like(var_dist_p)\n",
    "                final_p =  (perc_mtx-np.abs(min_dist_p)) * var_dist_p + min_dist_p\n",
    "\n",
    "                var_dist = dist * final_p\n",
    "\n",
    "                val = curr_node.stats.X + var_dist\n",
    "                val = np.clip(val, test_fn.lb, test_fn.ub)\n",
    "\n",
    "                vector = RealVector(val,\n",
    "                                    lb=test_fn.lb, ub=test_fn.ub,\n",
    "                                    )\n",
    "                node = TreeNode(vector, test_fn, parent=curr_node, \n",
    "                                identifier = curr_node.identifier + \"_001\" ,\n",
    "                                turbo_length = curr_node.turbo_length,\n",
    "                                uct_improve = node_uct_improve,\n",
    "                                )\n",
    "                var_norm = np.linalg.norm(var_dist, np.inf)\n",
    "                curr_node = node\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # opt curr_node\n",
    "            curr_node.visit_once()\n",
    "            _n = curr_node.parent\n",
    "            while _n:\n",
    "                _n.visit_once()\n",
    "                _n = _n.parent\n",
    "\n",
    "            curr_node.stp_alpha = 1./(curr_node.level+1) * width / np.sqrt(curr_node.visit)\n",
    "            if curr_node.y_best > stp_switch_thres :\n",
    "                curr_node.opt_node(max_budget = iter_descent,\n",
    "                            use_alpha= True,\n",
    "                            use_mask = False,\n",
    "                            )\n",
    "            else:\n",
    "                curr_node.opt_node(opt='zigzag',\n",
    "                                max_budget = iter_descent,\n",
    "                                use_alpha= True,\n",
    "                                    use_mask = False,\n",
    "                                    iteration = stp_internal_steps,\n",
    "                                    cumulate_improve_count = 10,\n",
    "                                    )\n",
    "            if iter_turbo >0:\n",
    "                curr_node.create_kids_turbo(iteration = iter_turbo)\n",
    "\n",
    "            np.savetxt(log, root.saved_y_X, header='y X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test functions definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Psz_4W1k9Lmd"
   },
   "source": [
    "## Synthetic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "e11A6XY89KhB"
   },
   "outputs": [],
   "source": [
    "class Ackley:\n",
    "    def __init__(self, dims=10):\n",
    "        self.dims      = dims\n",
    "        self.lb        = -5 * np.ones(dims)\n",
    "        self.ub        =  10 * np.ones(dims)\n",
    "        self.counter   = 0\n",
    "        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if isinstance(x, numbers.Number):\n",
    "            x = np.array([x])\n",
    "        if isinstance(x, list):\n",
    "            x = np.array(x)            \n",
    "        if not isinstance(x, np.ndarray):\n",
    "            x = x.X          \n",
    "\n",
    "        assert len(x) == self.dims\n",
    "        assert x.ndim == 1\n",
    "        x = np.clip(x, self.lb, self.ub)\n",
    "        assert np.all(x <= self.ub) and np.all(x >= self.lb)\n",
    "        \n",
    "        self.counter += 1\n",
    "        \n",
    "        result = (-20*np.exp(-0.2 * np.sqrt(np.inner(x,x) / x.size )) -np.exp(np.cos(2*np.pi*x).sum() /x.size) + 20 +np.e )\n",
    "        return result\n",
    "    \n",
    "    \n",
    "class Michalewicz:\n",
    "    def __init__(self, dims=2, m=10):\n",
    "        self.dims      = dims\n",
    "        self.lb        = np.zeros(self.dims)\n",
    "        self.ub        = np.pi * np.ones(self.dims)\n",
    "        self.m         = 10\n",
    "        self.counter   = 0\n",
    "        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if isinstance(x, numbers.Number):\n",
    "            x = np.array([x])\n",
    "        if isinstance(x, list):\n",
    "            x = np.array(x)            \n",
    "        if not isinstance(x, np.ndarray):\n",
    "            x = x.X          \n",
    "\n",
    "        assert len(x) == self.dims\n",
    "        assert x.ndim == 1\n",
    "        x = np.clip(x, self.lb, self.ub)\n",
    "        try:\n",
    "            assert np.all(x <= self.ub) and np.all(x >= self.lb)\n",
    "        except:\n",
    "            x = np.clip(x, self.lb, self.ub)\n",
    "        self.counter += 1\n",
    "               \n",
    "            \n",
    "        a_sin = np.sin(x)\n",
    "        b = x**2  * (np.arange(x.shape[0]) + 1) / np.pi\n",
    "        b_sin = np.sin(b)**(2*self.m)\n",
    "        \n",
    "        result = -np.sum(a_sin*b_sin)\n",
    "        return result    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAS problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB201_Cifar10:\n",
    "    \n",
    "    def __init__(self, file=None):\n",
    "        self.dims         = 6\n",
    "        self.lb           =  0.51 * np.ones(self.dims)\n",
    "        self.ub           =  5.49 * np.ones(self.dims)\n",
    "        self.counter      = 0\n",
    "        self.file         = file\n",
    "        if self.file is None:\n",
    "            self.file = './Data/Data_nb201_cifar10.pickle'\n",
    "        with open(self.file, 'rb') as fp:\n",
    "            self.data = pickle.load(fp)\n",
    "        \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        if isinstance(x, numbers.Number):\n",
    "            x = np.array([x])     \n",
    "        if isinstance(x, list):\n",
    "            x = np.array(x)            \n",
    "        if not isinstance(x, np.ndarray):\n",
    "            x = x.X  \n",
    "            \n",
    "            \n",
    "        self.counter += 1\n",
    "        assert len(x) == self.dims\n",
    "        assert x.ndim == 1\n",
    "        x = np.clip(x, self.lb, self.ub)\n",
    "        assert np.all(x <= self.ub) and np.all(x >= self.lb)\n",
    "        \n",
    "        \n",
    "        x_key = tuple(np.rint(x).astype(int))                \n",
    "        eval_loss = 100 - self.data[x_key]\n",
    "\n",
    "        return eval_loss\n",
    "\n",
    "\n",
    "class NB201_Cifar100:\n",
    "    \n",
    "    def __init__(self, file=None):\n",
    "        self.dims         = 6\n",
    "        self.lb           =  0.51 * np.ones(self.dims)\n",
    "        self.ub           =  5.49 * np.ones(self.dims)\n",
    "        self.counter      = 0\n",
    "        self.file         = file\n",
    "        if self.file is None:\n",
    "            self.file = './Data/Data_nb201_cifar100.pickle'\n",
    "            \n",
    "        with open(self.file, 'rb') as fp:\n",
    "            self.data = pickle.load(fp)\n",
    "        \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        if isinstance(x, numbers.Number):\n",
    "            x = np.array([x])     \n",
    "        if isinstance(x, list):\n",
    "            x = np.array(x)            \n",
    "        if not isinstance(x, np.ndarray):\n",
    "            x = x.X  \n",
    "            \n",
    "            \n",
    "        self.counter += 1\n",
    "        assert len(x) == self.dims\n",
    "        assert x.ndim == 1\n",
    "        x = np.clip(x, self.lb, self.ub)\n",
    "        assert np.all(x <= self.ub) and np.all(x >= self.lb)\n",
    "        \n",
    "        \n",
    "        x_key = tuple(np.rint(x).astype(int))                \n",
    "        eval_loss = 100 - self.data[x_key]\n",
    "\n",
    "        return eval_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdqLCk-B913A"
   },
   "source": [
    "# MCTD tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-CvCrYJDyDG"
   },
   "source": [
    "### Ackley 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2iR1usw_Dxwk",
    "outputId": "aa61667d-6f66-4370-d027-9d2f55ae0f4c"
   },
   "outputs": [],
   "source": [
    "for jj in range(5):\n",
    "    fn = Ackley\n",
    "    dims = 50\n",
    "\n",
    "    log = os.path.join(output_dir, fn.__name__ + str(dims) + '%02d'%jj+'.dat')\n",
    "\n",
    "\n",
    "    MCDesentSearch(fn, dims, log,\n",
    "                    max_steps = 10,\n",
    "                    iter_descent = 20,\n",
    "                    iter_turbo = 20,\n",
    "                    node_uct_improve = 10.,\n",
    "                    node_uct_explore = 0.5,\n",
    "                    leaf_improve = 50.,\n",
    "                    leaf_explore = 0.1,\n",
    "                    branch_explore = .1,\n",
    "                    width = 0.2,\n",
    "                    stp_switch_thres = 10.,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTkwPcfAezn1"
   },
   "source": [
    "### Ackley 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "wfdE1AJ4ewxm",
    "outputId": "05e055bb-70ba-429a-8281-42f8c920265f"
   },
   "outputs": [],
   "source": [
    "for jj in range(5):\n",
    "    fn = Ackley\n",
    "    dims = 100\n",
    "\n",
    "    log = os.path.join(output_dir, fn.__name__ + str(dims) + '%02d'%jj+'.dat')\n",
    "\n",
    "\n",
    "    MCDesentSearch(fn, dims, log,\n",
    "                    max_steps = 80,\n",
    "                    iter_descent = 20,\n",
    "                    iter_turbo = 20,\n",
    "                    node_uct_improve = 20.,\n",
    "                    node_uct_explore = 0.5,\n",
    "                    leaf_improve = 5.,\n",
    "                    leaf_explore = 0.1,\n",
    "                    branch_explore = .1,\n",
    "                    width = 0.2,\n",
    "                    stp_switch_thres = 4.,\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aZYf0CnfkOu"
   },
   "source": [
    "### Mich 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IVfS1b-Kewsq",
    "outputId": "f370d156-3fff-45a1-b35f-240ee27e823d"
   },
   "outputs": [],
   "source": [
    "for jj in range(5):\n",
    "    fn = Michalewicz\n",
    "    dims = 100\n",
    "\n",
    "    log = os.path.join(output_dir, fn.__name__ + '%02d'%jj+'.dat')\n",
    "\n",
    "\n",
    "    MCDesentSearch(fn, dims, log,\n",
    "                    max_steps = 100,\n",
    "                    iter_descent = 10,\n",
    "                    iter_turbo = 20,\n",
    "                    node_uct_improve = 50.,\n",
    "                    node_uct_explore = 1.,\n",
    "                    leaf_improve = 1.,\n",
    "                    leaf_explore = 10.,  \n",
    "                    branch_explore = .2,\n",
    "                    width = 0.02,\n",
    "                    stp_switch_thres = -30.,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAB-Cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jj in range(5):\n",
    "    fn = NB201_Cifar10\n",
    "    log = os.path.join(output_dir, fn.__name__ + '%02d'%jj+'.dat')\n",
    "\n",
    "    MCDesentSearch(fn, log,\n",
    "                    max_steps = 100,\n",
    "                    iter_descent = 5,\n",
    "                    iter_turbo = 20,\n",
    "                    node_uct_improve = 50.,\n",
    "                    node_uct_explore = 1.,\n",
    "                    leaf_improve = 100.,\n",
    "                    leaf_explore = 10.,\n",
    "                    branch_explore = 1.0,\n",
    "                    stp_switch_thres = 5.,\n",
    "                    width = 0.5,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAB-Cifar-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jj in range(5):\n",
    "    fn = NB201_Cifar100\n",
    "    log = os.path.join(output_dir, fn.__name__ + '%02d'%jj+'.dat')\n",
    "\n",
    "    MCDesentSearch(fn, log,\n",
    "                    max_steps = 100,\n",
    "                    iter_descent = 5,\n",
    "                    iter_turbo = 20,\n",
    "                    node_uct_improve = 50.,\n",
    "                    node_uct_explore = 1.,\n",
    "                    leaf_improve = 100.,\n",
    "                    leaf_explore = 10.,\n",
    "                    branch_explore = 1.0,\n",
    "                    stp_switch_thres = 5.,\n",
    "                    width = 0.5,\n",
    "                  )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "name": "MyMain_Muj2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:ptgpu]",
   "language": "python",
   "name": "conda-env-ptgpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
